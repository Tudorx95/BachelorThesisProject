"""
Template pentru antrenarea reÈ›elelor neuronale Ã®n medii Federated Learning
Compatibil cu orice arhitecturÄƒ TensorFlow/Keras

Model: ResNet18 PRE-ANTRENAT descÄƒrcat de pe HuggingFace Hub
Dataset: CIFAR-10 (60,000 imagini 32x32 RGB, 10 clase)
Ideal pentru: testarea atacurilor de tip data poisoning È™i backdoor attacks

IMPORTANT: Acest script DESCARCÄ‚ modelul pre-antrenat (nu-l creeazÄƒ de la 0)
"""

import tensorflow as tf
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from typing import Tuple, Dict, Any
import json
import os


# ============================================================================
# CONFIGURAÈšIE GLOBALÄ‚ - CIFAR-10
# ============================================================================

CIFAR10_CLASSES = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]
NUM_CLASSES = 10
IMG_SIZE = (32, 32)  # CIFAR-10 native resolution

# CONFIGURARE HUGGINGFACE
HUGGINGFACE_REPO_ID = "Tudorx95/resnet18-cifar10"  # âœï¸ ÃNLOCUIEÈ˜TE CU REPO-UL TÄ‚U
MODEL_FILENAME = "ResNet18_CIFAR10.keras"  # âœï¸ ÃNLOCUIEÈ˜TE CU NUMELE FIÈ˜IERULUI TÄ‚U


# ============================================================================
# 1. FUNCÈšIE PENTRU EXTRAGEREA DATELOR (CIFAR-10)
# ============================================================================

def load_train_test_data() -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    ÃncarcÄƒ CIFAR-10 dataset folosind tf.keras.datasets.
    
    Returns:
        Tuple[tf.data.Dataset, tf.data.Dataset]: (train_dataset, test_dataset)
    """
    print("\nğŸ“¦ DescÄƒrcare CIFAR-10 dataset...")
    
    # DescarcÄƒ CIFAR-10 (descÄƒrcare automatÄƒ la prima rulare)
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    
    print(f"   âœ“ Train: {len(x_train)} imagini")
    print(f"   âœ“ Test: {len(x_test)} imagini")
    
    # ConverteÈ™te Ã®n tf.data.Dataset
    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    
    return train_ds, test_ds


def preprocess(image, label):
    """
    Preprocesare de bazÄƒ pentru imagini È™i label-uri CIFAR-10.
    Aceasta este funcÈ›ia folositÄƒ Ã®n FL simulation.
    """
    # Normalizare [0, 1]
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.ensure_shape(image, (*IMG_SIZE, 3))
    
    # Flatten label dacÄƒ e 2D
    if len(label.shape) > 0:
        label = tf.squeeze(label)
    
    # One-hot encoding
    label = tf.cast(label, tf.int32)
    label = tf.one_hot(label, NUM_CLASSES)
    
    return image, label


def preprocess_loaded_data(train_ds, test_ds) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    PreproceseazÄƒ dataset-urile Ã®ncÄƒrcate.
    
    Args:
        train_ds: Dataset de antrenare brut
        test_ds: Dataset de testare brut
    
    Returns:
        Tuple de dataset-uri preprocesate
    """
    train_ds = train_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
    test_ds = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
    return train_ds, test_ds


def load_client_data(data_path: str, batch_size: int = 32) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    FuncÈ›ie pentru Ã®ncÄƒrcarea datelor Ã®n FL simulator (AGNOSTIC).
    
    AceastÄƒ funcÈ›ie este apelatÄƒ de fd_simulator pentru fiecare client.
    Implementarea este COMPLETÄ‚ - simulatorul NU aplicÄƒ nicio preprocesare proprie!
    
    Args:
        data_path: Path cÄƒtre directorul cu date
        batch_size: Dimensiunea batch-ului
        
    Returns:
        Tuple[tf.data.Dataset, tf.data.Dataset]: (train_ds, test_ds) complet preprocesate
    """
    from pathlib import Path
    
    data_path = Path(data_path)
    train_dir = data_path / "train"
    test_dir = data_path / "test"
    
    if not train_dir.exists() or not test_dir.exists():
        raise FileNotFoundError(f"Data directories not found: {data_path}")
    
    # ÃncarcÄƒ datele din directoare
    train_ds = tf.keras.utils.image_dataset_from_directory(
        train_dir,
        image_size=IMG_SIZE,
        batch_size=batch_size,
        color_mode='rgb',
        label_mode='int',
        shuffle=True,
        seed=42
    )
    
    test_ds = tf.keras.utils.image_dataset_from_directory(
        test_dir,
        image_size=IMG_SIZE,
        batch_size=batch_size,
        color_mode='rgb',
        label_mode='int',
        shuffle=False
    )
    
    # AplicÄƒ preprocesare COMPLETÄ‚
    def preprocess_for_fl(image, label):
        """Preprocesare specificÄƒ FL - normalizare + one-hot encoding."""
        image = tf.cast(image, tf.float32) / 255.0
        label = tf.cast(label, tf.int32)
        label = tf.one_hot(label, NUM_CLASSES)
        return image, label
    
    train_ds = train_ds.map(preprocess_for_fl, num_parallel_calls=tf.data.AUTOTUNE)
    test_ds = test_ds.map(preprocess_for_fl, num_parallel_calls=tf.data.AUTOTUNE)
    
    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
    
    return train_ds, test_ds


def download_data(output_dir: str = "cifar10_data"):
    """
    DescarcÄƒ, preproceseazÄƒ È™i salveazÄƒ datele CIFAR-10.
    AceastÄƒ funcÈ›ie este apelatÄƒ de orchestrator.
    """
    import numpy as np
    from pathlib import Path
    from PIL import Image

    output_path = Path(output_dir)
    train_dir = output_path / "train"
    test_dir = output_path / "test"
    train_dir.mkdir(parents=True, exist_ok=True)
    test_dir.mkdir(parents=True, exist_ok=True)

    print(f"Downloading and saving data to {output_dir}...")

    train_ds, test_ds = load_train_test_data()
    train_ds, test_ds = preprocess_loaded_data(train_ds, test_ds)

    X_train, y_train = [], []
    X_test, y_test = [], []

    for batch_images, batch_labels in train_ds:
        X_train.append(batch_images.numpy())
        y_train.append(batch_labels.numpy())

    for batch_images, batch_labels in test_ds:
        X_test.append(batch_images.numpy())
        y_test.append(batch_labels.numpy())

    X_train = np.concatenate(X_train, axis=0)
    y_train = np.concatenate(y_train, axis=0)
    X_test = np.concatenate(X_test, axis=0)
    y_test = np.concatenate(y_test, axis=0)

    if y_train.ndim > 1:
        y_train = np.argmax(y_train, axis=1)
    if y_test.ndim > 1:
        y_test = np.argmax(y_test, axis=1)

    def save_images(X, y, base_dir):
        for i, (img_array, label) in enumerate(zip(X, y)):
            class_name = CIFAR10_CLASSES[int(label)]
            class_dir = base_dir / class_name
            class_dir.mkdir(parents=True, exist_ok=True)
            img = (img_array * 255).astype(np.uint8)
            img_pil = Image.fromarray(img)
            img_pil.save(class_dir / f"{i:05d}.jpg")

    print("Saving training images...")
    save_images(X_train, y_train, train_dir)
    print("Saving test images...")
    save_images(X_test, y_test, test_dir)

    metadata = {
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "input_shape": list(X_train[0].shape),
        "num_classes": int(NUM_CLASSES),
        "class_names": CIFAR10_CLASSES,
        "dataset": "CIFAR-10"
    }

    with open(output_path / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ“ Data saved successfully!")
    return metadata


# ============================================================================
# 2. FUNCÈšIE PENTRU ANTRENAREA REÈšELEI NEURONALE
# ============================================================================

def train_neural_network(
    model: tf.keras.Model,
    train_dataset: tf.data.Dataset,
    validation_dataset: tf.data.Dataset = None,
    epochs: int = 10,
    callbacks: list = None,
    verbose: int = 1
) -> Dict[str, Any]:
    """
    AntreneazÄƒ o reÈ›ea neuronalÄƒ pe un dataset furnizat.
    FuncÈ›ie genericÄƒ compatibilÄƒ cu orice arhitecturÄƒ Keras.
    """
    if not isinstance(model, tf.keras.Model):
        raise TypeError("Modelul trebuie sÄƒ fie o instanÈ›Äƒ tf.keras.Model")
    
    if not hasattr(model, 'optimizer') or model.optimizer is None:
        raise ValueError("Modelul trebuie sÄƒ fie compilat Ã®nainte de antrenare.")
    
    if callbacks is None:
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss' if validation_dataset else 'loss',
                patience=5,
                restore_best_weights=True,
                verbose=1
            )
        ]
    
    history = model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=verbose
    )
    
    return history.history


# ============================================================================
# 3. FUNCÈšII PENTRU EXTRAGEREA PONDERILOR
# ============================================================================

def get_model_weights(model: tf.keras.Model):
    """Extrage ponderile modelului sub formÄƒ de listÄƒ de array-uri numpy."""
    return model.get_weights()


def set_model_weights(model: tf.keras.Model, weights) -> None:
    """SeteazÄƒ ponderile modelului din listÄƒ de array-uri numpy."""
    model.set_weights(weights)


# ============================================================================
# 4. FUNCÈšII PENTRU CALCULAREA METRICILOR
# ============================================================================

def calculate_metrics(
    model: tf.keras.Model,
    test_dataset: tf.data.Dataset,
    average: str = 'macro'
) -> Dict[str, float]:
    """CalculeazÄƒ metricile de evaluare pe dataset de test."""
    y_true_list = []
    y_pred_list = []
    
    for images, labels in test_dataset:
        predictions = model.predict(images, verbose=0)
        y_pred = np.argmax(predictions, axis=1)
        y_true = np.argmax(labels.numpy(), axis=1)
        y_true_list.extend(y_true)
        y_pred_list.extend(y_pred)
    
    y_true = np.array(y_true_list)
    y_pred = np.array(y_pred_list)
    
    metrics = {
        'accuracy': float(accuracy_score(y_true, y_pred)),
        'precision': float(precision_score(y_true, y_pred, average=average, zero_division=0)),
        'recall': float(recall_score(y_true, y_pred, average=average, zero_division=0)),
        'f1_score': float(f1_score(y_true, y_pred, average=average, zero_division=0))
    }
    
    return metrics


# ============================================================================
# 5. FUNCÈšII PENTRU SALVARE/ÃNCÄ‚RCARE CONFIGURAÈšIE MODEL
# ============================================================================

def save_model_config(
    model: tf.keras.Model,
    filepath: str,
    save_weights: bool = True
) -> None:
    """SalveazÄƒ configuraÈ›ia completÄƒ a modelului Ã®n format .keras."""
    if not filepath.endswith('.keras'):
        filepath += '.keras'
    
    if save_weights:
        model.save(filepath)
    else:
        model_json = model.to_json()
        config = {'architecture': model_json, 'config': model.get_config()}
        with open(filepath.replace('.keras', '_config.json'), 'w') as f:
            json.dump(config, f, indent=2)
    
    print(f"Model salvat Ã®n: {filepath}")


def load_model_config(filepath: str) -> tf.keras.Model:
    """ÃncarcÄƒ configuraÈ›ia modelului din fiÈ™ier .keras."""
    if not filepath.endswith('.keras'):
        filepath += '.keras'
    
    try:
        model = tf.keras.models.load_model(filepath)
        print(f"Model Ã®ncÄƒrcat din: {filepath}")
        return model
    except Exception as e:
        print(f"Eroare la Ã®ncÄƒrcarea modelului: {e}")
        raise


# ============================================================================
# 6. FUNCÈšIE AUXILIARÄ‚ PENTRU SALVAREA PONDERILOR SEPARATE
# ============================================================================

def save_weights_only(model: tf.keras.Model, filepath: str) -> None:
    """SalveazÄƒ doar ponderile modelului (fÄƒrÄƒ arhitecturÄƒ)."""
    model.save_weights(filepath)
    print(f"Ponderi salvate Ã®n: {filepath}")


def load_weights_only(model: tf.keras.Model, filepath: str) -> None:
    """ÃncarcÄƒ doar ponderile Ã®n model (arhitectura trebuie sÄƒ existe deja)."""
    model.load_weights(filepath)
    print(f"Ponderi Ã®ncÄƒrcate din: {filepath}")


# ============================================================================
# 7. FUNCÈšIE DE VALIDARE A MODELULUI
# ============================================================================

def validate_model_structure(model: tf.keras.Model) -> Dict[str, Any]:
    """ValideazÄƒ È™i returneazÄƒ informaÈ›ii despre structura modelului."""
    info = {
        'total_params': model.count_params(),
        'trainable_params': sum([tf.size(w).numpy() for w in model.trainable_weights]),
        'layers_count': len(model.layers),
        'input_shape': model.input_shape,
        'output_shape': model.output_shape,
        'is_compiled': hasattr(model, 'optimizer') and model.optimizer is not None
    }
    
    if hasattr(model, 'optimizer') and model.optimizer is not None:
        info['optimizer'] = model.optimizer.__class__.__name__
        info['loss'] = model.loss.__class__.__name__ if hasattr(model.loss, '__class__') else str(model.loss)
    
    return info


# ============================================================================
# 8. FUNCÈšII DE CONFIGURARE
# ============================================================================

def _model_compile(model: tf.keras.Model) -> tf.keras.Model:
    """CompileazÄƒ modelul cu setÄƒrile specificate de user."""
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model


def get_loss_type() -> str:
    """ReturneazÄƒ tipul funcÈ›iei de loss folosite."""
    return 'categorical_crossentropy'


def get_image_format() -> dict:
    """ReturneazÄƒ formatul imaginilor aÈ™teptat de model."""
    return {'channels': 3, 'size': IMG_SIZE}


def get_data_preprocessing() -> callable:
    """ReturneazÄƒ funcÈ›ia de preprocesare a datelor."""
    return preprocess


# ============================================================================
# 9. DESCÄ‚RCARE MODEL PRE-ANTRENAT DE PE HUGGINGFACE
# ============================================================================

def create_model():
    """
    DescarcÄƒ model PRE-ANTRENAT de pe HuggingFace Hub.
    
    IMPORTANT: Acest script NU creeazÄƒ modelul de la 0, ci Ã®l DESCARCÄ‚!
    
    Pentru a folosi aceastÄƒ funcÈ›ie:
    1. ÃncarcÄƒ modelul tÄƒu .keras pe HuggingFace Hub
    2. ActualizeazÄƒ variabilele globale:
       - HUGGINGFACE_REPO_ID = "your-username/your-repo"
       - MODEL_FILENAME = "your-model.keras"
    3. (OpÈ›ional) SeteazÄƒ token pentru repo-uri private:
       export HUGGING_FACE_HUB_TOKEN="hf_xxxxx"
    
    Returns:
        Model Keras descÄƒrcat È™i gata de folosit
    """
    from huggingface_hub import hf_hub_download
    
    print("\nğŸ”½ DescÄƒrcare model PRE-ANTRENAT de pe HuggingFace...")
    print(f"   Repo: {HUGGINGFACE_REPO_ID}")
    print(f"   FiÈ™ier: {MODEL_FILENAME}")
    
    try:
        # ObÈ›ine token din environment (opÈ›ional, pentru repo-uri private)
        token = os.environ.get("HUGGING_FACE_HUB_TOKEN")
        
        if token:
            print(f"   ğŸ” Token gÄƒsit: hf_{'*' * 10}{token[-4:]}")
        else:
            print("   â„¹ï¸  FÄƒrÄƒ token (repo public)")
        
        # DescarcÄƒ modelul (cu cache local automat)
        print("   â³ DescÄƒrcare Ã®n curs...")
        
        model_path = hf_hub_download(
            repo_id=HUGGINGFACE_REPO_ID,
            filename=MODEL_FILENAME,
            cache_dir="./models",  # Cache local pentru reutilizare
            token=token,
            force_download=False  # FoloseÈ™te cache dacÄƒ existÄƒ
        )
        
        print(f"   âœ“ DescÄƒrcat Ã®n: {model_path}")
        
        # ÃncarcÄƒ modelul
        print("   ğŸ“‚ ÃncÄƒrcare model Ã®n memorie...")
        model = tf.keras.models.load_model(model_path)
        
        print(f"   âœ“ Model Ã®ncÄƒrcat cu succes!")
        print(f"   ğŸ“Š ArhitecturÄƒ: {model.name}")
        print(f"   ğŸ“Š Parametri: {model.count_params():,}")
        print(f"   ğŸ“Š Input shape: {model.input_shape}")
        print(f"   ğŸ“Š Output shape: {model.output_shape}")
        
        # VerificÄƒ dacÄƒ modelul e compilat
        if not hasattr(model, 'optimizer') or model.optimizer is None:
            print("\n   âš ï¸  Modelul nu este compilat. Compilare automatÄƒ...")
            _model_compile(model)
            print("   âœ“ Model compilat!")
        else:
            print(f"   âœ“ Model deja compilat (optimizer: {model.optimizer.__class__.__name__})")
        
        return model
        
    except Exception as e:
        print(f"\n   âŒ Eroare la descÄƒrcare model: {e}")
        print("\n   ğŸ’¡ VerificÄƒri necesare:")
        print(f"      1. Repo existÄƒ: https://huggingface.co/{HUGGINGFACE_REPO_ID}")
        print(f"      2. FiÈ™ier existÄƒ: {MODEL_FILENAME}")
        print("      3. Internet connection")
        
        if "401" in str(e) or "403" in str(e):
            print("\n      4. Pentru repo privat, seteazÄƒ token:")
            print("         export HUGGING_FACE_HUB_TOKEN='hf_xxxxx'")
        
        print("\n   ğŸ“ Setup rapid:")
        print(f"""
# 1. InstaleazÄƒ dependenÈ›e
pip install huggingface-hub tensorflow

# 2. (OpÈ›ional) Login pentru repo privat
huggingface-cli login

# 3. VerificÄƒ repo
huggingface-cli repo-info {HUGGINGFACE_REPO_ID}

# 4. ActualizeazÄƒ constante Ã®n script:
HUGGINGFACE_REPO_ID = "your-username/your-repo"
MODEL_FILENAME = "your-model.keras"
""")
        raise


# ============================================================================
# EXEMPLU DE UTILIZARE - CONTINUARE ANTRENARE
# ============================================================================

if __name__ == "__main__":
    """
    Exemplu complet: DescarcÄƒ model pre-antrenat È™i continuÄƒ antrenarea.
    """
    
    print("=" * 70)
    print("CONTINUARE ANTRENARE RESNET18 PE CIFAR-10")
    print(f"Model sursÄƒ: HuggingFace Hub ({HUGGINGFACE_REPO_ID})")
    print("=" * 70)
    
    # Pasul 1: ÃncÄƒrcare CIFAR-10
    print("\n[Pasul 1] ÃncÄƒrcare CIFAR-10 dataset...")
    train_ds, test_ds = load_train_test_data()
    train_ds, test_ds = preprocess_loaded_data(train_ds, test_ds)
    
    # Pasul 2: DescÄƒrcare model pre-antrenat
    print("\n[Pasul 2] DescÄƒrcare model pre-antrenat de pe HuggingFace...")
    model = create_model()
    
    # Pasul 3: Validare structurÄƒ
    print("\n[Pasul 3] Validare structurÄƒ model:")
    model_info = validate_model_structure(model)
    for key, value in model_info.items():
        print(f"   {key}: {value}")
    
    # Pasul 4: Evaluare ÃNAINTE de antrenare suplimentarÄƒ
    print("\n[Pasul 4] Evaluare model ÃNAINTE de fine-tuning...")
    metrics_before = calculate_metrics(model, test_ds)
    print("   Metrici iniÈ›iale (model pre-antrenat):")
    for metric_name, value in metrics_before.items():
        print(f"      {metric_name}: {value:.4f}")
    
    # # Pasul 5: Continuare antrenare (fine-tuning)
    # print("\n[Pasul 5] Continuare antrenare (fine-tuning cu 5 epoci)...")
    # print("   â„¹ï¸  Modelul este deja antrenat - facem doar ajustÄƒri fine")
    
    # # OpÈ›ional: Reduce learning rate pentru fine-tuning
    # model.compile(
    #     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Learning rate mai mic
    #     loss='categorical_crossentropy',
    #     metrics=['accuracy']
    # )
    
    # history = train_neural_network(
    #     model=model,
    #     train_dataset=train_ds,
    #     validation_dataset=test_ds,
    #     epochs=5,  # PuÈ›ine epoci pentru fine-tuning
    #     verbose=1
    # )
    
    # # Pasul 6: Evaluare DUPÄ‚ antrenare suplimentarÄƒ
    # print("\n[Pasul 6] Evaluare model DUPÄ‚ fine-tuning...")
    # metrics_after = calculate_metrics(model, test_ds)
    # print("   Metrici finale:")
    # for metric_name, value in metrics_after.items():
    #     print(f"      {metric_name}: {value:.4f}")
    
    # # ComparaÈ›ie
    # print("\n   ğŸ“ˆ ÃmbunÄƒtÄƒÈ›ire:")
    # for metric_name in metrics_before.keys():
    #     diff = metrics_after[metric_name] - metrics_before[metric_name]
    #     symbol = "ğŸ“ˆ" if diff > 0 else "ğŸ“‰" if diff < 0 else "â¡ï¸"
    #     print(f"      {metric_name}: {diff:+.4f} {symbol}")
    
    # # Pasul 7: Extragere ponderi
    # print("\n[Pasul 7] Extragere ponderi...")
    # weights = get_model_weights(model)
    # print(f"   NumÄƒr de layere cu ponderi: {len(weights)}")
    
    # # Pasul 8: Salvare model actualizat
    print("\n[Pasul 8] Salvare model actualizat...")
    filepath = f"{model.name}_.keras"
    save_model_config(model, filepath)
    
    # print("\n" + "=" * 70)
    # print("âœ“ FINE-TUNING FINALIZAT CU SUCCES!")
    # print("=" * 70)
    # print(f"\nğŸ“¦ Model actualizat salvat: {filepath}")
    # print(f"ğŸ“Š Parametri: {model.count_params():,}")
    
    # print("\nğŸ’¡ UrmÄƒtorii paÈ™i:")
    # print("   1. (OpÈ›ional) Upload modelul actualizat pe HuggingFace:")
    # print(f"      python upload_to_huggingface.py {filepath} {HUGGINGFACE_REPO_ID}")
    # print("\n   2. FoloseÈ™te modelul Ã®n simulator-ul FL")
    
    # print("\nğŸ”¬ Pentru teste de data poisoning:")
    # print("   â€¢ Modelul pre-antrenat este ideal ca baseline")
    # print("   â€¢ TesteazÄƒ robusteÈ›ea la atacuri backdoor")
    # print("   â€¢ Framework: ART (Adversarial Robustness Toolbox)")
    
    # print("\nFinish")