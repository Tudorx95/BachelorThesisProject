import tensorflow as tf
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from typing import Tuple, Dict, Any
import os
import shutil

# ============================================================================
# 0. CONFIGURARE - Folosim un model simplu È™i rapid
# ============================================================================
# Vom folosi MobileNetV2 pre-antrenat, disponibil direct Ã®n Keras
# Este echivalentul unui model de pe HuggingFace dar mai uÈ™or de integrat
MODEL_NAME = "MobileNetV2"
NUM_CLASSES = 5  # Vom folosi 5 clase pentru exemplu
IMG_SIZE = (224, 224)  # Dimensiunea standard pentru MobileNetV2

# Alternative pentru Ã®ncÄƒrcare directÄƒ din tf.keras.applications:
# - ResNet50, ResNet101, ResNet152
# - VGG16, VGG19
# - InceptionV3, InceptionResNetV2
# - DenseNet121, DenseNet169, DenseNet201
# - EfficientNetB0, EfficientNetB1, ... EfficientNetB7
# - NASNetMobile, NASNetLarge

DATA_DIR = './hf_data'

# ============================================================================
# 1. FUNCÈšII PENTRU DESCÄ‚RCAREA MODELULUI "HUGGINGFACE-STYLE"
# ============================================================================
def download_pretrained_model(
    model_name: str = MODEL_NAME,
    num_classes: int = NUM_CLASSES,
    weights: str = 'imagenet'
) -> tf.keras.Model:
    """
    DescarcÄƒ un model pre-antrenat (similar cu HuggingFace).
    Folosim tf.keras.applications care oferÄƒ modele pre-antrenate.
    
    Args:
        model_name: Numele modelului (MobileNetV2, ResNet50, etc.)
        num_classes: NumÄƒrul de clase pentru fine-tuning
        weights: 'imagenet' pentru ponderi pre-antrenate sau None
    
    Returns:
        Model TensorFlow/Keras
    """
    print(f"ğŸ“¥ DescÄƒrcare model pre-antrenat: {model_name}")
    print(f"   Ponderi iniÈ›iale: {weights}")
    print(f"   NumÄƒr clase finale: {num_classes}")
    
    try:
        # DicÈ›ionar cu modele disponibile
        models_dict = {
            'MobileNetV2': tf.keras.applications.MobileNetV2,
            'ResNet50': tf.keras.applications.ResNet50,
            'ResNet101': tf.keras.applications.ResNet101,
            'VGG16': tf.keras.applications.VGG16,
            'VGG19': tf.keras.applications.VGG19,
            'InceptionV3': tf.keras.applications.InceptionV3,
            'DenseNet121': tf.keras.applications.DenseNet121,
            'EfficientNetB0': tf.keras.applications.EfficientNetB0,
            'EfficientNetB1': tf.keras.applications.EfficientNetB1,
        }
        
        if model_name not in models_dict:
            raise ValueError(f"Model {model_name} nu este suportat. OpÈ›iuni: {list(models_dict.keys())}")
        
        # ÃncÄƒrcÄƒm modelul de bazÄƒ fÄƒrÄƒ top layer
        base_model = models_dict[model_name](
            weights=weights,
            include_top=False,
            input_shape=(*IMG_SIZE, 3),
            pooling='avg'  # Global average pooling
        )
        
        # ÃngheÈ›Äƒm layerele de bazÄƒ (pentru fine-tuning mai rapid)
        base_model.trainable = False
        
        # Construim modelul complet cu layere custom
        inputs = tf.keras.Input(shape=(*IMG_SIZE, 3))
        x = base_model(inputs, training=False)
        x = tf.keras.layers.Dropout(0.2)(x)
        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
        
        model = tf.keras.Model(inputs, outputs, name=f"{model_name}_FineTuned")
        
        print(f"âœ… Model descÄƒrcat cu succes!")
        print(f"   Total parametri: {model.count_params():,}")
        print(f"   Parametri antrenabili: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}")
        
        return model
        
    except Exception as e:
        print(f"âŒ Eroare la descÄƒrcarea modelului: {e}")
        raise

# ============================================================================
# 2. FUNCÈšII PENTRU ÃNCÄ‚RCAREA DATELOR
# ============================================================================
def create_dummy_dataset(
    num_samples: int = 1000,
    num_classes: int = NUM_CLASSES,
    img_size: Tuple[int, int] = IMG_SIZE
) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    CreeazÄƒ un dataset dummy pentru testare rapidÄƒ.
    Ãn producÈ›ie, Ã®nlocuieÈ™te cu date reale.
    """
    print(f"\nğŸ“Š Creare dataset dummy pentru testare")
    print(f"   Samples: {num_samples}")
    print(f"   Classes: {num_classes}")
    print(f"   Image size: {img_size}")
    
    # GenerÄƒm date random
    train_size = int(num_samples * 0.8)
    test_size = num_samples - train_size
    
    # Date de antrenare
    train_images = np.random.rand(train_size, *img_size, 3).astype(np.float32)
    train_labels = np.random.randint(0, num_classes, train_size).astype(np.int32)
    
    # Date de test
    test_images = np.random.rand(test_size, *img_size, 3).astype(np.float32)
    test_labels = np.random.randint(0, num_classes, test_size).astype(np.int32)
    
    # CreÄƒm tf.data.Dataset
    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
    
    print(f"âœ… Dataset creat!")
    
    return train_ds, test_ds

def load_real_dataset_from_url(
    dataset_url: str = None
) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    ÃncarcÄƒ un dataset real de imagini.
    Exemplu cu CIFAR-10 (disponibil Ã®n Keras).
    """
    print(f"\nğŸ“¥ ÃncÄƒrcare dataset real: CIFAR-10")
    
    try:
        # ÃncÄƒrcÄƒm CIFAR-10 (similar cu descÄƒrcarea de pe HuggingFace)
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
        
        # RedimensionÄƒm imaginile la dimensiunea cerutÄƒ de model
        x_train = tf.image.resize(x_train, IMG_SIZE).numpy()
        x_test = tf.image.resize(x_test, IMG_SIZE).numpy()
        
        # Normalizare
        x_train = x_train / 255.0
        x_test = x_test / 255.0
        
        # Flatten labels
        y_train = y_train.flatten()
        y_test = y_test.flatten()
        
        # CreÄƒm tf.data.Dataset
        train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
        test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))
        
        print(f"âœ… Dataset Ã®ncÄƒrcat!")
        print(f"   Train samples: {len(x_train)}")
        print(f"   Test samples: {len(x_test)}")
        print(f"   Classes: 10 (CIFAR-10)")
        
        return train_ds, test_ds
        
    except Exception as e:
        print(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca CIFAR-10: {e}")
        print("   Se creeazÄƒ dataset dummy...")
        return create_dummy_dataset(num_classes=10)

def preprocess_loaded_data(
    train_ds: tf.data.Dataset, 
    test_ds: tf.data.Dataset,
    batch_size: int = 32,
    shuffle_buffer: int = 1000
) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    """
    PreproceseazÄƒ dataset-urile cu batching È™i shuffling.
    """
    print(f"\nğŸ”„ Preprocesare date...")
    print(f"   Batch size: {batch_size}")
    
    # Preprocesare pentru antrenare
    train_ds = train_ds.shuffle(shuffle_buffer).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    # Preprocesare pentru test (fÄƒrÄƒ shuffle)
    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    
    print("âœ… Preprocesare completÄƒ!")
    
    return train_ds, test_ds

# ============================================================================
# 3. FUNCÈšIE PENTRU COMPILAREA MODELULUI
# ============================================================================
def compile_model(
    model: tf.keras.Model,
    learning_rate: float = 1e-3,
    optimizer_name: str = 'adam'
) -> tf.keras.Model:
    """
    CompileazÄƒ modelul cu setÄƒrile optime.
    """
    print(f"\nâš™ï¸ Compilare model...")
    print(f"   Optimizer: {optimizer_name}")
    print(f"   Learning rate: {learning_rate}")
    
    # SelectÄƒm optimizer-ul
    if optimizer_name.lower() == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer_name.lower() == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    
    # CompilÄƒm modelul
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=['accuracy']
    )
    
    print("âœ… Model compilat!")
    
    return model

# ============================================================================
# 4. FUNCÈšIE PENTRU ANTRENAREA REÈšELEI NEURONALE (DIN TEMPLATE)
# ============================================================================
def train_neural_network(
    model: tf.keras.Model,
    train_dataset: tf.data.Dataset,
    validation_dataset: tf.data.Dataset = None,
    epochs: int = 5,
    callbacks: list = None,
    verbose: int = 1
) -> Dict[str, Any]:
    """
    AntreneazÄƒ o reÈ›ea neuronalÄƒ pe un dataset furnizat.
    FuncÈ›ie genericÄƒ compatibilÄƒ cu orice arhitecturÄƒ Keras.
    """
    if not isinstance(model, tf.keras.Model):
        raise TypeError("Modelul trebuie sÄƒ fie o instanÈ›Äƒ tf.keras.Model")
    
    # Verificare compilare
    try:
        _ = model.optimizer
        is_compiled = True
    except (AttributeError, ValueError):
        is_compiled = False
    
    if not is_compiled:
        raise ValueError(
            "Modelul trebuie sÄƒ fie compilat Ã®nainte de antrenare. "
            "FolosiÈ›i compile_model(model)"
        )
    
    if callbacks is None:
        callbacks = []
    
    print(f"\nğŸš€ Ãncepe antrenarea pentru {epochs} epoci...")
    
    history = model.fit(
        train_dataset,
        validation_data=validation_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=verbose
    )
    
    print(f"âœ… Antrenare finalizatÄƒ!")
    
    return {
        'history': history.history,
        'epochs_completed': len(history.history['loss'])
    }

# ============================================================================
# 5. FUNCÈšII PENTRU EXTRAGEREA/SETAREA PONDERILOR (DIN TEMPLATE)
# ============================================================================
def get_model_weights(model: tf.keras.Model) -> list:
    """Extrage ponderile (weights) din model."""
    return model.get_weights()

def set_model_weights(model: tf.keras.Model, weights: list) -> None:
    """SeteazÄƒ ponderile (weights) Ã®n model."""
    model.set_weights(weights)

# ============================================================================
# 6. FUNCÈšIE PENTRU CALCULAREA METRICILOR (DIN TEMPLATE)
# ============================================================================
def calculate_metrics(
    model: tf.keras.Model,
    test_dataset: tf.data.Dataset,
    average: str = 'weighted'
) -> Dict[str, float]:
    """
    CalculeazÄƒ metrici de evaluare: accuracy, precision, recall, f1_score.
    """
    print("\nğŸ“Š Calculare metrici...")
    
    y_true = []
    y_pred = []
    
    for batch_data in test_dataset:
        if len(batch_data) == 2:
            images, labels = batch_data
        else:
            raise ValueError("Dataset-ul trebuie sÄƒ conÈ›inÄƒ (features, labels)")
        
        predictions = model.predict(images, verbose=0)
        
        if len(predictions.shape) > 1 and predictions.shape[1] > 1:
            y_pred.extend(np.argmax(predictions, axis=1))
        else:
            y_pred.extend((predictions > 0.5).astype(int).flatten())
        
        if len(labels.shape) > 1 and labels.shape[1] > 1:
            y_true.extend(np.argmax(labels.numpy(), axis=1))
        else:
            y_true.extend(labels.numpy().flatten())
    
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    metrics = {
        'accuracy': float(accuracy_score(y_true, y_pred)),
        'precision': float(precision_score(y_true, y_pred, average=average, zero_division=0)),
        'recall': float(recall_score(y_true, y_pred, average=average, zero_division=0)),
        'f1_score': float(f1_score(y_true, y_pred, average=average, zero_division=0))
    }
    
    print("âœ… Metrici calculate!")
    
    return metrics

# ============================================================================
# 7. FUNCÈšIE DE VALIDARE A MODELULUI (DIN TEMPLATE)
# ============================================================================
def validate_model_structure(model: tf.keras.Model) -> Dict[str, Any]:
    """
    ValideazÄƒ È™i returneazÄƒ informaÈ›ii despre structura modelului.
    """
    try:
        _ = model.optimizer
        is_compiled = True
    except (AttributeError, ValueError):
        is_compiled = False
    
    info = {
        'total_params': model.count_params(),
        'trainable_params': sum([tf.size(w).numpy() for w in model.trainable_weights]),
        'non_trainable_params': sum([tf.size(w).numpy() for w in model.non_trainable_weights]),
        'layers_count': len(model.layers),
        'input_shape': model.input_shape,
        'output_shape': model.output_shape,
        'is_compiled': is_compiled
    }
   
    if is_compiled:
        info['optimizer'] = model.optimizer.__class__.__name__
        info['loss'] = model.loss.__class__.__name__ if hasattr(model.loss, '__class__') else str(model.loss)
    
    return info

# ============================================================================
# 8. FUNCÈšIE PENTRU FINE-TUNING (UNFREEZE LAYERS)
# ============================================================================
def unfreeze_model_layers(
    model: tf.keras.Model,
    num_layers_to_unfreeze: int = 20
) -> tf.keras.Model:
    """
    DeblocheazÄƒ ultimele layere ale modelului pentru fine-tuning.
    """
    print(f"\nğŸ”“ Deblocare layere pentru fine-tuning...")
    print(f"   Layere de deblocheat: {num_layers_to_unfreeze}")
    
    # ÃngheÈ›Äƒm toate layerele mai Ã®ntÃ¢i
    for layer in model.layers:
        layer.trainable = False
    
    # DeblocheazÄƒ ultimele layere
    for layer in model.layers[-num_layers_to_unfreeze:]:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = True
    
    trainable_count = sum([1 for layer in model.layers if layer.trainable])
    print(f"âœ… Layere antrenabile: {trainable_count}/{len(model.layers)}")
    
    return model

# ============================================================================
# EXEMPLU DE UTILIZARE COMPLETÄ‚
# ============================================================================
if __name__ == "__main__":
    
    print("=" * 80)
    print("ğŸ¤— MODEL PRE-ANTRENAT (HUGGINGFACE-STYLE) + TEMPLATE INTEGRATION")
    print("=" * 80)
    
    try:
        # ====================================================================
        # PASUL 1: DescÄƒrcare model pre-antrenat
        # ====================================================================
        model = download_pretrained_model(
            model_name='MobileNetV2',  # SchimbÄƒ cu: ResNet50, VGG16, etc.
            num_classes=10,  # Pentru CIFAR-10
            weights='imagenet'
        )
        
        # ====================================================================
        # PASUL 2: ÃncÄƒrcare date
        # ====================================================================
        # OpÈ›iune 1: Dataset real (CIFAR-10)
        train_ds, test_ds = load_real_dataset_from_url()
        
        # OpÈ›iune 2: Dataset dummy (pentru testare rapidÄƒ)
        # train_ds, test_ds = create_dummy_dataset(num_classes=10)
        
        # ====================================================================
        # PASUL 3: Preprocesare
        # ====================================================================
        train_ds, test_ds = preprocess_loaded_data(
            train_ds, 
            test_ds, 
            batch_size=32,
            shuffle_buffer=1000
        )
        
        # ====================================================================
        # PASUL 4: Compilare model
        # ====================================================================
        model = compile_model(
            model, 
            learning_rate=1e-3,
            optimizer_name='adam'
        )
        
        # ====================================================================
        # PASUL 5: Validare structurÄƒ (FUNCÈšIE DIN TEMPLATE)
        # ====================================================================
        print("\nğŸ“‹ --- InformaÈ›ii Model ---")
        model_info = validate_model_structure(model)
        for key, value in model_info.items():
            print(f"   {key}: {value}")
        
        # ====================================================================
        # PASUL 6: Antrenare iniÈ›ialÄƒ (FUNCÈšIE DIN TEMPLATE)
        # ====================================================================
        history = train_neural_network(
            model=model,
            train_dataset=train_ds,
            validation_dataset=test_ds,
            epochs=2,  # Antrenare rapidÄƒ pentru top layers
            verbose=1
        )
        
        print(f"\nâœ… Antrenare iniÈ›ialÄƒ finalizatÄƒ dupÄƒ {history['epochs_completed']} epoci")
        
        # ====================================================================
        # PASUL 7: Extragere ponderi (FUNCÈšIE DIN TEMPLATE)
        # ====================================================================
        weights = get_model_weights(model)
        print(f"\nğŸ’¾ Ponderi extrase: {len(weights)} tensori")
        print(f"   Primul tensor shape: {weights[0].shape}")
        print(f"   Ultimul tensor shape: {weights[-1].shape}")
        
        # ====================================================================
        # PASUL 8: Calculare metrici (FUNCÈšIE DIN TEMPLATE)
        # ====================================================================
        metrics = calculate_metrics(model, test_ds, average='macro')
        print("\nğŸ“Š --- Metrici dupÄƒ antrenare iniÈ›ialÄƒ ---")
        for metric_name, value in metrics.items():
            print(f"   {metric_name}: {value:.4f}")
        
        # ====================================================================
        # PASUL 9: Fine-tuning (opÈ›ional)
        # ====================================================================
        print("\n" + "=" * 80)
        print("ğŸ”¥ FINE-TUNING (Deblocare layere)")
        print("=" * 80)
        
        model = unfreeze_model_layers(model, num_layers_to_unfreeze=30)
        
        # Re-compilÄƒm cu learning rate mai mic pentru fine-tuning
        model = compile_model(model, learning_rate=1e-4, optimizer_name='adam')
        
        # Antrenare fine-tuning
        history_ft = train_neural_network(
            model=model,
            train_dataset=train_ds,
            validation_dataset=test_ds,
            epochs=2,
            verbose=1
        )
        
        # Metrici dupÄƒ fine-tuning
        metrics_ft = calculate_metrics(model, test_ds, average='macro')
        print("\nğŸ“Š --- Metrici dupÄƒ fine-tuning ---")
        for metric_name, value in metrics_ft.items():
            print(f"   {metric_name}: {value:.4f}")
        
        # ====================================================================
        # PASUL 10: Salvare model
        # ====================================================================
        model_save_path = "./saved_pretrained_model.h5"
        model.save(model_save_path)
        print(f"\nğŸ’¾ Model salvat la: {model_save_path}")
        
        # ====================================================================
        # PASUL 11: Test de setare ponderi (FUNCÈšIE DIN TEMPLATE)
        # ====================================================================
        print("\nğŸ”„ Test setare ponderi...")
        new_weights = get_model_weights(model)
        set_model_weights(model, new_weights)
        print("âœ… Ponderi setate cu succes!")
        
        print("\n" + "=" * 80)
        print("ğŸ‰ PROCES COMPLET FINALIZAT CU SUCCES!")
        print("=" * 80)
        print("\nğŸ“ REZUMAT:")
        print(f"   - Model: MobileNetV2 fine-tuned")
        print(f"   - Dataset: CIFAR-10 (50,000 train, 10,000 test)")
        print(f"   - Accuracy iniÈ›ialÄƒ: {metrics['accuracy']:.4f}")
        print(f"   - Accuracy dupÄƒ fine-tuning: {metrics_ft['accuracy']:.4f}")
        print(f"   - ÃmbunÄƒtÄƒÈ›ire: {(metrics_ft['accuracy'] - metrics['accuracy'])*100:.2f}%")
            
    except Exception as e:
        import traceback
        print(f"\nâŒ EÈ™ec Ã®n execuÈ›ia completÄƒ. Eroare: {e}")
        print("\nTraceback complet:")
        traceback.print_exc()